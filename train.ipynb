{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/TFG/Pruebas ejecucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
    "!pip uninstall -y -q tensorflow keras tensorflow-estimator tensorflow-text\n",
    "!pip install -q -U tensorflow-text tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing import create_datasets, text_vectorization, process_text\n",
    "from model import Transformer\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'd_model': d_model, #check this, with self.d_model raises an error\n",
    "            'warmup_steps': self.warmup_steps,\n",
    "        }\n",
    "        return config\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "    mask = label != 0\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits = True,\n",
    "        reduction = 'none',\n",
    "    )\n",
    "    loss = loss_object(label, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_acc(label, pred):\n",
    "    pred = tf.argmax(pred, axis = 2)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    match = label == pred\n",
    "\n",
    "    mask = label != 0\n",
    "    match = match & mask\n",
    "\n",
    "    match = tf.cast(match, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return  tf.reduce_sum(match) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = create_datasets()\n",
    "context_text_processor_train, target_text_processor_train = text_vectorization(train)\n",
    "context_text_processor_val, target_text_processor_val = text_vectorization(val)\n",
    "context_text_processor_test, target_text_processor_test = text_vectorization(test)\n",
    "\n",
    "# print(context_text_processor.get_vocabulary()[:10])\n",
    "# print(target_text_processor.get_vocabulary()[:10])\n",
    "\n",
    "train_ds = train.map(lambda x, y: process_text(x, y, context_text_processor_train, target_text_processor_train), tf.data.AUTOTUNE)\n",
    "val_ds = val.map(lambda x, y: process_text(x, y, context_text_processor_val, target_text_processor_val), tf.data.AUTOTUNE)\n",
    "test_ds = val.map(lambda x, y: process_text(x, y, context_text_processor_test, target_text_processor_test), tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Model\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers = num_layers,\n",
    "    d_model = d_model,\n",
    "    num_heads = num_heads,\n",
    "    dff = dff,\n",
    "    input_vocab_size = context_text_processor_train.vocabulary_size(),\n",
    "    target_vocab_size = target_text_processor_train.vocabulary_size(),\n",
    "    dropout_rate = dropout_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Loss and Optimizer\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "my_optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Compile\n",
    "transformer.compile(\n",
    "    loss = masked_loss,\n",
    "    optimizer=my_optimizer,\n",
    "    metrics = [masked_acc],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.fit(\n",
    "    train_ds,\n",
    "    epochs = 10,\n",
    "    validation_data = val_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.save('my_model_16032023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstructed_model = keras.models.load_model(\"my_model_16032023\", custom_objects={ 'CustomSchedule' : CustomSchedule, 'masked_loss': masked_loss, 'masked_acc': masked_acc })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.testing.assert_allclose(\n",
    "#     transformer.predict(val_ds), reconstructed_model.predict(val_ds)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstructed_model.fit(\n",
    "#     test_ds,\n",
    "#     epochs = 3,\n",
    "#     validation_data = val_ds,\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNF5FhiIGxP4FRH6JVpoAeC",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
