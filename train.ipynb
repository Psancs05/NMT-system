{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/TFG/Pruebas ejecucion\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/TFG/Pruebas ejecucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following packages will be REMOVED:\n",
      "  libcudnn8-dev\n",
      "The following held packages will be changed:\n",
      "  libcudnn8\n",
      "The following packages will be DOWNGRADED:\n",
      "  libcudnn8\n",
      "0 upgraded, 0 newly installed, 1 downgraded, 1 to remove and 21 not upgraded.\n",
      "Need to get 430 MB of archives.\n",
      "After this operation, 1,153 MB disk space will be freed.\n",
      "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  libcudnn8 8.1.0.77-1+cuda11.2 [430 MB]\n",
      "Fetched 430 MB in 5s (91.5 MB/s)\n",
      "(Reading database ... 128285 files and directories currently installed.)\n",
      "Removing libcudnn8-dev (8.7.0.84-1+cuda11.8) ...\n",
      "update-alternatives: removing manually selected alternative - switching libcudnn to auto mode\n",
      "\u001b[1mdpkg:\u001b[0m \u001b[1;33mwarning:\u001b[0m downgrading libcudnn8 from 8.7.0.84-1+cuda11.8 to 8.1.0.77-1+cuda11.2\n",
      "(Reading database ... 128252 files and directories currently installed.)\n",
      "Preparing to unpack .../libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb ...\n",
      "Unpacking libcudnn8 (8.1.0.77-1+cuda11.2) over (8.7.0.84-1+cuda11.8) ...\n",
      "Setting up libcudnn8 (8.1.0.77-1+cuda11.2) ...\n",
      "\u001b[33mWARNING: Skipping tensorflow-text as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 KB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
    "!pip uninstall -y -q tensorflow keras tensorflow-estimator tensorflow-text\n",
    "!pip install -q -U tensorflow-text==2.11.0 tensorflow==2.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing import create_datasets, text_vectorization, process_text\n",
    "from model import Transformer, CustomSchedule, masked_loss, masked_acc\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = create_datasets()\n",
    "context_text_processor, target_text_processor = text_vectorization(train)\n",
    "\n",
    "# print(context_text_processor.get_vocabulary()[:10])\n",
    "# print(target_text_processor.get_vocabulary()[:10])\n",
    "\n",
    "train_ds = train.map(lambda x, y: process_text(x, y, context_text_processor, target_text_processor), tf.data.AUTOTUNE)\n",
    "val_ds = val.map(lambda x, y: process_text(x, y, context_text_processor, target_text_processor), tf.data.AUTOTUNE)\n",
    "test_ds = val.map(lambda x, y: process_text(x, y, context_text_processor, target_text_processor), tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics(history):\n",
    "    try:\n",
    "        metrics = pd.read_csv('metrics.csv')\n",
    "    except:\n",
    "        metrics = pd.DataFrame(columns = ['loss', 'masked_acc', 'val_loss', 'val_masked_acc'])\n",
    "\n",
    "    new_metrics = pd.DataFrame(history.history)\n",
    "    metrics = pd.concat([metrics, new_metrics], ignore_index = True)\n",
    "\n",
    "    metrics.to_csv('metrics.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Model\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers = num_layers,\n",
    "    d_model = d_model,\n",
    "    num_heads = num_heads,\n",
    "    dff = dff,\n",
    "    input_vocab_size = context_text_processor.vocabulary_size(),\n",
    "    target_vocab_size = target_text_processor.vocabulary_size(),\n",
    "    dropout_rate = dropout_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Loss and Optimizer\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "my_optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Compile\n",
    "transformer.compile(\n",
    "    loss = masked_loss,\n",
    "    optimizer=my_optimizer,\n",
    "    metrics = [masked_acc],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "820/820 [==============================] - 237s 239ms/step - loss: 5.7028 - masked_acc: 0.2074 - val_loss: 4.3551 - val_masked_acc: 0.2866\n",
      "Epoch 2/10\n",
      "820/820 [==============================] - 175s 213ms/step - loss: 3.9948 - masked_acc: 0.3158 - val_loss: 3.5934 - val_masked_acc: 0.3497\n",
      "Epoch 3/10\n",
      "820/820 [==============================] - 174s 212ms/step - loss: 3.3826 - masked_acc: 0.3682 - val_loss: 3.0406 - val_masked_acc: 0.4119\n",
      "Epoch 4/10\n",
      "820/820 [==============================] - 173s 211ms/step - loss: 2.9185 - masked_acc: 0.4228 - val_loss: 2.7592 - val_masked_acc: 0.4483\n",
      "Epoch 5/10\n",
      "820/820 [==============================] - 172s 209ms/step - loss: 2.6258 - masked_acc: 0.4619 - val_loss: 2.5359 - val_masked_acc: 0.4812\n",
      "Epoch 6/10\n",
      "820/820 [==============================] - 172s 209ms/step - loss: 2.4149 - masked_acc: 0.4936 - val_loss: 2.3380 - val_masked_acc: 0.5183\n",
      "Epoch 7/10\n",
      "820/820 [==============================] - 171s 208ms/step - loss: 2.2337 - masked_acc: 0.5225 - val_loss: 2.2602 - val_masked_acc: 0.5311\n",
      "Epoch 8/10\n",
      "820/820 [==============================] - 171s 209ms/step - loss: 2.1060 - masked_acc: 0.5429 - val_loss: 2.1648 - val_masked_acc: 0.5491\n",
      "Epoch 9/10\n",
      "820/820 [==============================] - 170s 208ms/step - loss: 2.0098 - masked_acc: 0.5585 - val_loss: 2.1062 - val_masked_acc: 0.5595\n",
      "Epoch 10/10\n",
      "820/820 [==============================] - 171s 209ms/step - loss: 1.9312 - masked_acc: 0.5711 - val_loss: 2.0933 - val_masked_acc: 0.5642\n"
     ]
    }
   ],
   "source": [
    "history = transformer.fit(\n",
    "    train_ds,\n",
    "    epochs = 10,\n",
    "    validation_data = val_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import io\n",
    "# import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_tokenizer_json = context_text_processor.\n",
    "# with io.open('context_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "#     f.write(json.dumps(context_tokenizer_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, positional_embedding_2_layer_call_fn, positional_embedding_2_layer_call_and_return_conditional_losses, dropout_26_layer_call_fn, dropout_26_layer_call_and_return_conditional_losses while saving (showing 5 of 357). These functions will not be directly callable after loading.\n",
      "/usr/local/lib/python3.9/dist-packages/keras/saving/legacy/saved_model/layer_serialization.py:134: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return serialization.serialize_keras_object(obj)\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "transformer.save('my_model_28032023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_model = keras.models.load_model(\"my_model_28032023\", custom_objects={ 'CustomSchedule' : CustomSchedule, 'masked_loss': masked_loss, 'masked_acc': masked_acc })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.testing.assert_allclose(\n",
    "#     transformer.predict(val_ds), reconstructed_model.predict(val_ds)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "820/820 [==============================] - 225s 234ms/step - loss: 1.7069 - masked_acc: 0.6100 - val_loss: 1.9951 - val_masked_acc: 0.5852\n",
      "Epoch 2/5\n",
      "820/820 [==============================] - 174s 212ms/step - loss: 1.6891 - masked_acc: 0.6140 - val_loss: 2.0178 - val_masked_acc: 0.5805\n",
      "Epoch 3/5\n",
      "820/820 [==============================] - 171s 209ms/step - loss: 1.7266 - masked_acc: 0.6062 - val_loss: 2.0596 - val_masked_acc: 0.5723\n",
      "Epoch 4/5\n",
      "820/820 [==============================] - 172s 210ms/step - loss: 1.7895 - masked_acc: 0.5935 - val_loss: 2.1015 - val_masked_acc: 0.5669\n",
      "Epoch 5/5\n",
      "820/820 [==============================] - 172s 210ms/step - loss: 1.8597 - masked_acc: 0.5809 - val_loss: 2.1397 - val_masked_acc: 0.5561\n"
     ]
    }
   ],
   "source": [
    "history = reconstructed_model.fit(\n",
    "    train_ds,\n",
    "    epochs = 5,\n",
    "    validation_data = val_ds,\n",
    ")\n",
    "create_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_translation(sentence, translated_sentence, ground_truth):\n",
    "  print(f'{\"Input:\":15s}: {sentence}')\n",
    "  print(f'{\"Prediction\":15s}: {translated_sentence}')\n",
    "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize_sentence(processor, sentence):\n",
    "  output_sentence = tf.constant('')\n",
    "  vocab = processor.get_vocabulary()\n",
    "\n",
    "  for token in sentence:\n",
    "    output_sentence = tf.strings.join([output_sentence, vocab[token]], separator=' ')\n",
    "                                        \n",
    "  sentence = output_sentence.numpy().decode('utf-8')\n",
    "  sentence = sentence.replace('[START]', '').replace('[END]', '').strip()\n",
    "  \n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, context_processor, target_processor, model, max_tokens=128):\n",
    "  # Convert the sentence to tensor\n",
    "  sentence = tf.constant(sentence)\n",
    "  assert isinstance(sentence, tf.Tensor)\n",
    "\n",
    "  # Give the correct shape\n",
    "  if len(sentence.shape) == 0:\n",
    "    sentence = sentence[tf.newaxis]\n",
    "  \n",
    "  # Tokenize the sentence\n",
    "  sentence = context_processor(sentence).to_tensor()\n",
    "\n",
    "  # Input for the encoder\n",
    "  encoder_input = sentence\n",
    "  \n",
    "  # Input for the decoder\n",
    "  start_end_tokens = target_processor([''])[0]\n",
    "  start_token = start_end_tokens[0][tf.newaxis]\n",
    "  end_token = start_end_tokens[-1][tf.newaxis]\n",
    "\n",
    "  # Convert decoder sentence to TensorArray for feed the model\n",
    "  output_tensor = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "  output_tensor = output_tensor.write(0, start_token)\n",
    "  \n",
    "  # seq_to_seq generation\n",
    "  for i in tf.range(max_tokens):\n",
    "    # Prepare the output tensor\n",
    "    output = tf.transpose(output_tensor.stack())\n",
    "    \n",
    "    # Get the model predictions\n",
    "    predictions = model((encoder_input, output), training=False)\n",
    "    \n",
    "    # Select the last token from the seq_len dimension\n",
    "    predictions = predictions[:, -1, :] # Shape (batch_size, 1, vocab_size)\n",
    "    # Get the most probable next token\n",
    "    predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "    # Concatenate the predicted token to the output sentence\n",
    "    output_tensor = output_tensor.write(i+1, predicted_id) #TODO: en el tuto es predicted_id[0]\n",
    "\n",
    "    # Check if the prediction is fully generated with the last token\n",
    "    if predicted_id == end_token:\n",
    "      break\n",
    "  \n",
    "  # Convert the output to the correct shape\n",
    "  output = tf.transpose(output_tensor.stack()) # Shape (1, tokens_generated)\n",
    "\n",
    "  # Detokenize the output sentence\n",
    "  text = detokenize_sentence(target_processor, output[0])\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : I want to be at home today\n",
      "Prediction     : quiero estar en casa hoy\n",
      "Ground truth   : Quiero estar en casa hoy\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I want to be at home today'\n",
    "ground_truth = 'Quiero estar en casa hoy'\n",
    "\n",
    "translated_text = translate(sentence, context_text_processor, target_text_processor, reconstructed_model)\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : I like to drink water in the park\n",
      "Prediction     : me gusta beber agua en el parque\n",
      "Ground truth   : Me gusta beber agua en el parque\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I like to drink water in the park'\n",
    "ground_truth = 'Me gusta beber agua en el parque'\n",
    "\n",
    "translated_text = translate(sentence, context_text_processor, target_text_processor, reconstructed_model)\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : university has taught me that life is not only about technical knowledge, but also about meeting people who can help you in the future.\n",
      "Prediction     : la universidad me ha [UNK] que la vida no solo sobre conocimiento tecnica sino tambien sobre las personas que pueden [UNK] en el futuro .\n",
      "Ground truth   : la universidad me ha enseñado que en la vida no solo importan los conocimientos técnicos, sino también conocer personas que te puedan ayudar en el futuro\n"
     ]
    }
   ],
   "source": [
    "sentence = 'university has taught me that life is not only about technical knowledge, but also about meeting people who can help you in the future.'\n",
    "ground_truth = 'la universidad me ha enseñado que en la vida no solo importan los conocimientos técnicos, sino también conocer personas que te puedan ayudar en el futuro'\n",
    "\n",
    "translated_text = translate(sentence, context_text_processor, target_text_processor, reconstructed_model)\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP2GzR+HwjmcAgH6sHTTTUm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
