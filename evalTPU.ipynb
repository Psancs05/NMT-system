{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting tensorflow-text\n",
      "  Downloading tensorflow_text-2.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorflow<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text) (2.12.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text) (0.13.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (23.3.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (4.5.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.54.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (0.32.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (2.3.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (16.0.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (0.4.8)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.14.1)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (2.12.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (2.12.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.22.4)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (0.4.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (67.7.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (3.20.3)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (2.12.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.6.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (3.8.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.13,>=2.12.0->tensorflow-text) (0.40.0)\n",
      "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow-text) (1.10.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow-text) (0.1.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (3.4.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (1.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (0.7.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.17.3)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (0.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (3.2.2)\n",
      "Installing collected packages: tensorflow-text\n",
      "Successfully installed tensorflow-text-2.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/TFG/Pruebas ejecucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Transformer, CustomSchedule, masked_loss, masked_acc\n",
    "from data_preprocessing import create_datasets, text_vectorization, process_text\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "import nltk.translate.bleu_score as bleu\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'models/en_es4_256_512_8_0.2.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect TPU and create a TPU strategy\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "strategy = tf.distribute.TPUStrategy(resolver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = create_datasets()\n",
    "context_text_processor, target_text_processor = text_vectorization(train)\n",
    "\n",
    "# Wrap model creation and batch_translate function inside the strategy scope\n",
    "with strategy.scope():\n",
    "    #! Model\n",
    "    num_layers = 6\n",
    "    d_model = 256\n",
    "    dff = 512\n",
    "    num_heads = 8\n",
    "    dropout_rate = 0.3\n",
    "\n",
    "    learning_rate = CustomSchedule(d_model)\n",
    "    my_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "    model = Transformer(\n",
    "        num_layers = num_layers,\n",
    "        d_model = d_model,\n",
    "        num_heads = num_heads,\n",
    "        dff = dff,\n",
    "        input_vocab_size = context_text_processor.vocabulary_size(),\n",
    "        target_vocab_size = target_text_processor.vocabulary_size(),\n",
    "        dropout_rate = dropout_rate,\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss = masked_loss,\n",
    "        optimizer=my_optimizer,\n",
    "        metrics = [masked_acc],\n",
    "    )\n",
    "\n",
    "# Llama al modelo con datos ficticios para construirlo\n",
    "dummy_input = tf.ones((1, 1), dtype=tf.int32)\n",
    "dummy_context = tf.ones((1, 1), dtype=tf.int32)\n",
    "model((dummy_context, dummy_input), training=False)\n",
    "\n",
    "model.load_weights(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_translation(sentence, translated_sentence, ground_truth):\n",
    "  print(f'{\"Input:\":15s}: {sentence}')\n",
    "  print(f'{\"Prediction\":15s}: {translated_sentence}')\n",
    "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize_sentence(processor, sentence):\n",
    "  output_sentence = tf.constant('')\n",
    "  vocab = processor.get_vocabulary()\n",
    "  lookup = tf.constant('')\n",
    "\n",
    "  for token in sentence:\n",
    "    output_sentence = tf.strings.join([output_sentence, vocab[token]], separator=' ')\n",
    "\n",
    "  lookup = tf.strings.split(output_sentence, sep=' ')\n",
    "\n",
    "  sentence = output_sentence.numpy().decode('utf-8')\n",
    "  sentence = sentence.replace('[START]', '').replace('[END]', '').strip()\n",
    "\n",
    "  return sentence, lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, context_processor, target_processor, model, max_tokens=128):\n",
    "  # Convert the sentence to tensor\n",
    "  sentence = tf.constant([[sentence]])\n",
    "  assert isinstance(sentence, tf.Tensor)\n",
    "\n",
    "  # Give the correct shape\n",
    "  if len(sentence.shape) == 0:\n",
    "    sentence = sentence[tf.newaxis]\n",
    "  \n",
    "  # Tokenize the sentence\n",
    "  sentence = context_processor(sentence).to_tensor()\n",
    "\n",
    "  # Input for the encoder\n",
    "  encoder_input = sentence\n",
    "  \n",
    "  # Input for the decoder\n",
    "  start_end_tokens = target_processor([''])[0]\n",
    "  start_token = start_end_tokens[0][tf.newaxis]\n",
    "  end_token = start_end_tokens[-1][tf.newaxis]\n",
    "\n",
    "  # Convert decoder sentence to TensorArray for feed the model\n",
    "  output_tensor = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "  output_tensor = output_tensor.write(0, start_token)\n",
    "  \n",
    "  # seq_to_seq generation\n",
    "  for i in tf.range(max_tokens):\n",
    "    # Prepare the output tensor\n",
    "    output = tf.transpose(output_tensor.stack())\n",
    "    \n",
    "    # Get the model predictions\n",
    "    predictions = model((encoder_input, output), training=False)\n",
    "    \n",
    "    # Select the last token from the seq_len dimension\n",
    "    predictions = predictions[:, -1, :] # Shape (batch_size, 1, vocab_size)\n",
    "    # Get the most probable next token\n",
    "    predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "    # Concatenate the predicted token to the output sentence\n",
    "    output_tensor = output_tensor.write(i+1, predicted_id) #TODO: en el tuto es predicted_id[0]\n",
    "\n",
    "    # Check if the prediction is fully generated with the last token\n",
    "    if predicted_id == end_token:\n",
    "      break\n",
    "  \n",
    "  # Convert the output to the correct shape\n",
    "  output = tf.transpose(output_tensor.stack()) # Shape (1, tokens_generated)\n",
    "\n",
    "  # Detokenize the output sentence\n",
    "  text, lookup = detokenize_sentence(target_processor, output[0])\n",
    "\n",
    "  # 'tf.function' prevents us from using the attention_weights that were calculated on the\n",
    "  # last iteration of the loop. So, recalculate them outside the loop.\n",
    "  model((encoder_input, output[:, :-1]), training=False)\n",
    "  attention_weigths = model.decoder.last_attn_scores\n",
    "  #TODO: Check is attn_weigths is correct (with plotting function)\n",
    "\n",
    "  return text, lookup, attention_weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_translate_single_batch(sentences, context_processor, target_processor, model, max_tokens=128):\n",
    "    # Convert the sentences to a tensor\n",
    "    sentences = tf.constant(sentences)\n",
    "    assert isinstance(sentences, tf.Tensor)\n",
    "\n",
    "    # Tokenize the sentences\n",
    "    sentences = context_processor(sentences).to_tensor()\n",
    "\n",
    "    # Input for the encoder\n",
    "    encoder_input = sentences\n",
    "\n",
    "    # Input for the decoder\n",
    "    start_end_tokens = target_processor([''])[0]\n",
    "    start_token = start_end_tokens[0][tf.newaxis]\n",
    "    end_token = start_end_tokens[-1][tf.newaxis]\n",
    "    end_token_id = end_token.numpy()[0]\n",
    "    dot_token_id = target_processor.get_vocabulary().index('.')\n",
    "\n",
    "    # Prepare the initial output tensor\n",
    "    batch_size = tf.shape(encoder_input)[0]\n",
    "    initial_output = tf.repeat(start_token, repeats=batch_size, axis=0)\n",
    "\n",
    "    # Initialize the output tensor as a list of empty TensorArrays, one for each sentence\n",
    "    output_tensors = [tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True) for _ in range(batch_size)]\n",
    "\n",
    "    # Write the start token to each TensorArray\n",
    "    for i, tensor_array in enumerate(output_tensors):\n",
    "        tensor_array = tensor_array.write(0, initial_output[i])\n",
    "        tensor_array.mark_used()\n",
    "   \n",
    "\n",
    "    # Initialize a boolean tensor to track the completed sentences\n",
    "    completed_sentences = tf.zeros(batch_size, dtype=tf.bool)\n",
    "\n",
    "    for _ in tf.range(max_tokens):\n",
    "        # Prepare the output tensor\n",
    "        max_sentence_length = max([tensor_array.size() for tensor_array in output_tensors])\n",
    "        padded_output_tensors = [tf.pad(tensor_array.stack(), [[0, max_sentence_length - tensor_array.size()]]) for tensor_array in output_tensors]\n",
    "        output = tf.stack(padded_output_tensors, axis=0)\n",
    "\n",
    "        # Get the model predictions\n",
    "        predictions = model((encoder_input, output), training=False)\n",
    "\n",
    "        # Select the last token from the seq_len dimension\n",
    "        predictions = predictions[:, -1, :]  # Shape (batch_size, 1, vocab_size)\n",
    "\n",
    "        # Get the most probable next token\n",
    "        predicted_ids = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "        # Concatenate the predicted token to the output sentence\n",
    "        for i, (tensor_array, predicted_id) in enumerate(zip(output_tensors, predicted_ids)):\n",
    "            if not completed_sentences[i]:\n",
    "                if predicted_id.numpy() == end_token_id or predicted_id == dot_token_id:\n",
    "                    completed_sentences = tf.where(tf.range(batch_size) == i, True, completed_sentences)\n",
    "                    output_tensors[i] = tensor_array.write(tensor_array.size(), predicted_id)\n",
    "                else:\n",
    "                    output_tensors[i] = tensor_array.write(tensor_array.size(), predicted_id)\n",
    "            else:\n",
    "                # Keep the end token if the sentence is already completed\n",
    "                output_tensors[i] = tensor_array.write(tensor_array.size(), end_token_id)\n",
    "\n",
    "        # Check if all the predictions are fully generated with the last token\n",
    "        if tf.reduce_all(completed_sentences):\n",
    "            break\n",
    "    \n",
    "\n",
    "    # Convert the output to the correct shape\n",
    "    output = tf.stack([tensor_array.stack() for tensor_array in output_tensors], axis=0)  # Shape (batch_size, tokens_generated)\n",
    "\n",
    "    # Detokenize the output sentences\n",
    "    texts, lookups = [], []\n",
    "    for i in range(batch_size):\n",
    "        text, lookup = detokenize_sentence(target_processor, output[i])\n",
    "        texts.append(text)\n",
    "        lookups.append(lookup)\n",
    "\n",
    "    # 'tf.function' prevents us from using the attention_weights that were calculated on the\n",
    "    # last iteration of the loop. So, recalculate them outside the loop.\n",
    "    model((encoder_input, output[:, :-1]), training=False)\n",
    "    attention_weights = model.decoder.last_attn_scores\n",
    "\n",
    "    return texts, lookups, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_translate(sentences, context_processor, target_processor, model, max_tokens=128, batch_size=16):\n",
    "    num_batches = len(sentences) // batch_size + int(len(sentences) % batch_size > 0)\n",
    "    all_translated_texts = []\n",
    "    # all_lookups = []\n",
    "    # all_attention_weights = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        print('Batch starting... ' + str(i+1) + '/' + str(num_batches))\n",
    "        batch_start = i * batch_size\n",
    "        batch_end = min((i + 1) * batch_size, len(sentences))\n",
    "        batch_sentences = sentences[batch_start:batch_end]\n",
    "        \n",
    "        # Call the previous batch_translate function with the smaller batch of sentences\n",
    "        translated_text, lookup, attention_weights = batch_translate_single_batch(batch_sentences, context_processor, target_processor, model, max_tokens)\n",
    "        \n",
    "        all_translated_texts.extend(translated_text)\n",
    "        # all_lookups.extend(lookup)\n",
    "        # all_attention_weights.append(attention_weights)\n",
    "\n",
    "    return all_translated_texts, _, __\n",
    "    # return all_translated_texts, all_lookups, all_attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = 'I want to be at home today'\n",
    "# ground_truth = 'Quiero estar en casa hoy'/\n",
    "\n",
    "# translated_text, lookup, attention_weigths = translate(sentence, context_text_processor, target_text_processor, model)\n",
    "# print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = 'I like to drink water in the park'\n",
    "# ground_truth = 'Me gusta beber agua en el parque'\n",
    "\n",
    "# translated_text, lookup, attention_weigths = translate(sentence, context_text_processor, target_text_processor, model)\n",
    "# print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = 'university has taught me that life is not only about technical knowledge, but also about meeting people who can help you in the future.'\n",
    "# ground_truth = 'la universidad me ha enseñado que en la vida no solo importan los conocimientos técnicos, sino también conocer personas que te puedan ayudar en el futuro'\n",
    "\n",
    "# translated_text, lookup, attention_weigths = translate(sentence, context_text_processor, target_text_processor, model)\n",
    "# print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(context_text_processor.get_vocabulary()[:10])\n",
    "# print(target_text_processor.get_vocabulary()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = ['I want to be at home today', 'I like to drink water in the park', 'university has taught me that life is not only about technical knowledge, but also about meeting people who can help you in the future.']\n",
    "# ground_truth = 'Quiero estar en casa hoy'\n",
    "\n",
    "# translated_text, lookup, attention_weigths = batch_translate(sentence, context_text_processor, target_text_processor, model)\n",
    "# print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open file E:\\TFG\\eval\n",
    "with open('eval/europarl-v7.es-en.es', 'r') as f:\n",
    "    data_es = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eval/europarl-v7.es-en.en', 'r') as f:\n",
    "    data_en = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu_score(references, predictions):\n",
    "    # Tokenize the sentences\n",
    "    tokenized_references = [nltk.word_tokenize(ref) for ref in references]\n",
    "    tokenized_predictions = [nltk.word_tokenize(pred) for pred in predictions]\n",
    "\n",
    "    # Calculate the BLEU score using the NLTK library\n",
    "    bleu_score = bleu.corpus_bleu([[ref] for ref in tokenized_references], tokenized_predictions)\n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data_en[-128:]\n",
    "b = data_es[-128:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_sentences, _, __ = batch_translate(a, context_text_processor, target_text_processor, model, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_score = calculate_bleu_score(b, translated_sentences)\n",
    "print(\"BLEU Score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esp = [x.lower() for x in b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_score = calculate_bleu_score(esp, translated_sentences)\n",
    "print(\"BLEU Score:\", bleu_score)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
