{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/TFG/Pruebas ejecucion\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/TFG/Pruebas ejecucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following packages will be REMOVED:\n",
      "  libcudnn8-dev\n",
      "The following held packages will be changed:\n",
      "  libcudnn8\n",
      "The following packages will be DOWNGRADED:\n",
      "  libcudnn8\n",
      "0 upgraded, 0 newly installed, 1 downgraded, 1 to remove and 21 not upgraded.\n",
      "Need to get 430 MB of archives.\n",
      "After this operation, 1,153 MB disk space will be freed.\n",
      "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  libcudnn8 8.1.0.77-1+cuda11.2 [430 MB]\n",
      "Fetched 430 MB in 5s (81.8 MB/s)\n",
      "(Reading database ... 128276 files and directories currently installed.)\n",
      "Removing libcudnn8-dev (8.7.0.84-1+cuda11.8) ...\n",
      "update-alternatives: removing manually selected alternative - switching libcudnn to auto mode\n",
      "\u001b[1mdpkg:\u001b[0m \u001b[1;33mwarning:\u001b[0m downgrading libcudnn8 from 8.7.0.84-1+cuda11.8 to 8.1.0.77-1+cuda11.2\n",
      "(Reading database ... 128243 files and directories currently installed.)\n",
      "Preparing to unpack .../libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb ...\n",
      "Unpacking libcudnn8 (8.1.0.77-1+cuda11.2) over (8.7.0.84-1+cuda11.8) ...\n",
      "Setting up libcudnn8 (8.1.0.77-1+cuda11.2) ...\n",
      "\u001b[33mWARNING: Skipping tensorflow-text as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 KB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
    "!pip uninstall -y -q tensorflow keras tensorflow-estimator tensorflow-text\n",
    "!pip install -q -U tensorflow-text tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing import create_datasets, text_vectorization, process_text\n",
    "from model import Transformer,  CustomSchedule, masked_loss, masked_acc\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics(history):\n",
    "    try:\n",
    "        metrics = pd.read_csv('metrics.csv')\n",
    "    except:\n",
    "        metrics = pd.DataFrame(columns = ['loss', 'masked_acc', 'val_loss', 'val_masked_acc'])\n",
    "\n",
    "    new_metrics = pd.DataFrame(history.history)\n",
    "    metrics = pd.concat([metrics, new_metrics], ignore_index = True)\n",
    "\n",
    "    metrics.to_csv('metrics.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.init()\n",
    "# context_text_processor = utils.context_text_processor\n",
    "# target_text_processor = utils.target_text_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "train, val, test = create_datasets()\n",
    "context_text_processor, target_text_processor = text_vectorization(train)\n",
    "\n",
    "# print(context_text_processor.get_vocabulary()[:10])\n",
    "# print(target_text_processor.get_vocabulary()[:10])\n",
    "\n",
    "train_ds = train.map(lambda x, y: process_text(x, y, context_text_processor, target_text_processor), tf.data.AUTOTUNE)\n",
    "val_ds = val.map(lambda x, y: process_text(x, y, context_text_processor, target_text_processor), tf.data.AUTOTUNE)\n",
    "test_ds = val.map(lambda x, y: process_text(x, y, context_text_processor, target_text_processor), tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Model\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers = num_layers,\n",
    "    d_model = d_model,\n",
    "    num_heads = num_heads,\n",
    "    dff = dff,\n",
    "    input_vocab_size = context_text_processor.vocabulary_size(),\n",
    "    target_vocab_size = target_text_processor.vocabulary_size(),\n",
    "    dropout_rate = dropout_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Loss and Optimizer\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "my_optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Compile\n",
    "transformer.compile(\n",
    "    loss = masked_loss,\n",
    "    optimizer=my_optimizer,\n",
    "    metrics = [masked_acc],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "820/820 [==============================] - 337s 317ms/step - loss: 5.7152 - masked_acc: 0.2069 - val_loss: 4.3897 - val_masked_acc: 0.2840\n",
      "Epoch 2/10\n",
      "820/820 [==============================] - 199s 242ms/step - loss: 4.0151 - masked_acc: 0.3142 - val_loss: 3.5984 - val_masked_acc: 0.3494\n",
      "Epoch 3/10\n",
      "820/820 [==============================] - 185s 225ms/step - loss: 3.3867 - masked_acc: 0.3675 - val_loss: 3.0557 - val_masked_acc: 0.4087\n",
      "Epoch 4/10\n",
      "820/820 [==============================] - 180s 220ms/step - loss: 2.9118 - masked_acc: 0.4244 - val_loss: 2.7124 - val_masked_acc: 0.4575\n",
      "Epoch 5/10\n",
      "820/820 [==============================] - 183s 223ms/step - loss: 2.6236 - masked_acc: 0.4638 - val_loss: 2.5150 - val_masked_acc: 0.4888\n",
      "Epoch 6/10\n",
      "820/820 [==============================] - 184s 225ms/step - loss: 2.4144 - masked_acc: 0.4941 - val_loss: 2.3398 - val_masked_acc: 0.5202\n",
      "Epoch 7/10\n",
      "820/820 [==============================] - 180s 219ms/step - loss: 2.2335 - masked_acc: 0.5228 - val_loss: 2.2383 - val_masked_acc: 0.5352\n",
      "Epoch 8/10\n",
      "820/820 [==============================] - 179s 218ms/step - loss: 2.1070 - masked_acc: 0.5431 - val_loss: 2.1737 - val_masked_acc: 0.5484\n",
      "Epoch 9/10\n",
      "820/820 [==============================] - 177s 216ms/step - loss: 2.0089 - masked_acc: 0.5587 - val_loss: 2.1338 - val_masked_acc: 0.5566\n",
      "Epoch 10/10\n",
      "820/820 [==============================] - 178s 217ms/step - loss: 1.9303 - masked_acc: 0.5718 - val_loss: 2.0921 - val_masked_acc: 0.5635\n"
     ]
    }
   ],
   "source": [
    "history = transformer.fit(\n",
    "    train_ds,\n",
    "    epochs = 10,\n",
    "    validation_data = val_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import io\n",
    "# import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_tokenizer_json = context_text_processor.\n",
    "# with io.open('context_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "#     f.write(json.dumps(context_tokenizer_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, positional_embedding_layer_call_fn, positional_embedding_layer_call_and_return_conditional_losses, dropout_4_layer_call_fn, dropout_4_layer_call_and_return_conditional_losses while saving (showing 5 of 357). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ae3c260c642a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'my_model_21032023'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Save text processors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# with open('context_processor.pickle', 'wb') as handle:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/drive/MyDrive/TFG/Pruebas ejecucion/model.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         config = {\n\u001b[0;32m--> 301\u001b[0;31m             \u001b[0;34m'd_model'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#check this, with self.d_model raises an error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m             \u001b[0;34m'warmup_steps'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarmup_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "transformer.save('my_model_21032023')\n",
    "\n",
    "# Save text processors\n",
    "# with open('context_processor.pickle', 'wb') as handle:\n",
    "#   pickle.dump(context_text_processor, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('target_text_processor.pickle', 'wb') as handle:\n",
    "#   pickle.dump(target_text_processor, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_model = keras.models.load_model(\"my_model_16032023\", custom_objects={ 'CustomSchedule' : CustomSchedule, 'masked_loss': masked_loss, 'masked_acc': masked_acc })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.testing.assert_allclose(\n",
    "#     transformer.predict(val_ds), reconstructed_model.predict(val_ds)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstructed_model.fit(\n",
    "#     test_ds,\n",
    "#     epochs = 3,\n",
    "#     validation_data = val_ds,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_translation(sentence, translated_sentence, ground_truth):\n",
    "  print(f'{\"Input:\":15s}: {sentence}')\n",
    "  print(f'{\"Prediction\":15s}: {translated_sentence}')\n",
    "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize_sentence(processor, sentence):\n",
    "  output_sentence = tf.constant('')\n",
    "  vocab = processor.get_vocabulary()\n",
    "\n",
    "  for token in sentence:\n",
    "    output_sentence = tf.strings.join([output_sentence, vocab[token]], separator=' ')\n",
    "                                        \n",
    "  sentence = output_sentence.numpy().decode('utf-8')\n",
    "  sentence = sentence.replace('[START]', '').replace('[END]', '').strip()\n",
    "  \n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, context_processor, target_processor, model, max_tokens=128):\n",
    "  # Convert the sentence to tensor\n",
    "  sentence = tf.constant(sentence)\n",
    "  assert isinstance(sentence, tf.Tensor)\n",
    "\n",
    "  # Give the correct shape\n",
    "  if len(sentence.shape) == 0:\n",
    "    sentence = sentence[tf.newaxis]\n",
    "  \n",
    "  # Tokenize the sentence\n",
    "  sentence = context_processor(sentence).to_tensor()\n",
    "\n",
    "  # Input for the encoder\n",
    "  encoder_input = sentence\n",
    "  \n",
    "  # Input for the decoder\n",
    "  start_end_tokens = target_processor([''])[0]\n",
    "  start_token = start_end_tokens[0][tf.newaxis]\n",
    "  end_token = start_end_tokens[-1][tf.newaxis]\n",
    "\n",
    "  # Convert decoder sentence to TensorArray for feed the model\n",
    "  output_tensor = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "  output_tensor = output_tensor.write(0, start_token)\n",
    "  \n",
    "  # seq_to_seq generation\n",
    "  for i in tf.range(max_tokens):\n",
    "    # Prepare the output tensor\n",
    "    output = tf.transpose(output_tensor.stack())\n",
    "    \n",
    "    # Get the model predictions\n",
    "    predictions = model([encoder_input, output], training=False)\n",
    "    \n",
    "    # Select the last token from the seq_len dimension\n",
    "    predictions = predictions[:, -1, :] # Shape (batch_size, 1, vocab_size)\n",
    "    # Get the most probable next token\n",
    "    predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "    # Concatenate the predicted token to the output sentence\n",
    "    output_tensor = output_tensor.write(i+1, predicted_id) #TODO: en el tuto es predicted_id[0]\n",
    "\n",
    "    # Check if the prediction is fully generated with the last token\n",
    "    if predicted_id == end_token:\n",
    "      break\n",
    "  \n",
    "  # Convert the output to the correct shape\n",
    "  output = tf.transpose(output_tensor.stack()) # Shape (1, tokens_generated)\n",
    "\n",
    "  # Detokenize the output sentence\n",
    "  text = detokenize_sentence(target_processor, output[0])\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : I want to be at home today\n",
      "Prediction     : quiero estar en casa\n",
      "Ground truth   : Quiero estar en casa\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I want to be at home today'\n",
    "ground_truth = 'Quiero estar en casa'\n",
    "\n",
    "translated_text = translate(sentence, context_text_processor, target_text_processor, transformer)\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : I like to drink water in the park\n",
      "Prediction     : me gusta beber agua en el parque\n",
      "Ground truth   : Me gusta beber agua en el parque\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I like to drink water in the park'\n",
    "ground_truth = 'Me gusta beber agua en el parque'\n",
    "\n",
    "translated_text = translate(sentence, context_text_processor, target_text_processor, transformer)\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : university has taught me that life is not only about technical knowledge, but also about meeting people who can help you in the future.\n",
      "Prediction     : la universidad ha [UNK] que la vida no es solo acerca de conocimiento tecnica , sino tambien sobre la reunion personas que pueden ayudar en el futuro .\n",
      "Ground truth   : la universidad me ha enseñado que en la vida no solo importan los conocimientos técnicos, sino también conocer personas que te puedan ayudar en el futuro\n"
     ]
    }
   ],
   "source": [
    "sentence = 'university has taught me that life is not only about technical knowledge, but also about meeting people who can help you in the future.'\n",
    "ground_truth = 'la universidad me ha enseñado que en la vida no solo importan los conocimientos técnicos, sino también conocer personas que te puedan ayudar en el futuro'\n",
    "\n",
    "translated_text = translate(sentence, context_text_processor, target_text_processor, transformer)\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
