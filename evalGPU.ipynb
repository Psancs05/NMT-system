{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.10/dist-packages (2.12.1)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text) (0.13.0)\n",
      "Requirement already satisfied: tensorflow<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text) (2.12.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (23.3.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.54.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (3.8.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (0.4.10)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (2.12.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (16.0.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.22.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (67.7.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (2.12.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (2.12.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (4.5.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (0.32.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.13,>=2.12.0->tensorflow-text) (0.40.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow-text) (0.1.0)\n",
      "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow-text) (1.10.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.17.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (0.7.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/MyDrive/TFG/Pruebas ejecucion\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/TFG/Pruebas ejecucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import Transformer, CustomSchedule, masked_loss, masked_acc\n",
    "from data_preprocessing import create_datasets, text_vectorization, process_text, MAX_VOCAB_SIZE, standardize_text\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import nltk.translate.bleu_score as bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'weights/es_en__20_6_256_512_8_0-3.h5'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vocabulary\n",
    "with open('vocab/es_en_context_vocabulary.txt', 'r', encoding='utf-8') as f:\n",
    "    context_loaded_vocab = [line.strip() for line in f]\n",
    "\n",
    "# Load the vocabulary\n",
    "with open('vocab/es_en_target_vocabulary.txt', 'r', encoding='utf-8') as f:\n",
    "    target_loaded_vocab = [line.strip() for line in f]\n",
    "\n",
    "# Then create a new TextVectorization layer using the loaded vocabulary\n",
    "context_text_processor = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=MAX_VOCAB_SIZE,\n",
    "    standardize=standardize_text,\n",
    "    vocabulary=context_loaded_vocab,\n",
    "    ragged=True,\n",
    ")\n",
    "\n",
    "target_text_processor = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=MAX_VOCAB_SIZE,\n",
    "    standardize=standardize_text,\n",
    "    vocabulary=target_loaded_vocab,\n",
    "    ragged=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Model\n",
    "num_layers = 6\n",
    "d_model = 256\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.3\n",
    "\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "my_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "model = Transformer(\n",
    "    num_layers = num_layers,\n",
    "    d_model = d_model,\n",
    "    num_heads = num_heads,\n",
    "    dff = dff,\n",
    "    input_vocab_size = context_text_processor.vocabulary_size(),\n",
    "    target_vocab_size = target_text_processor.vocabulary_size(),\n",
    "    dropout_rate = dropout_rate,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss = masked_loss,\n",
    "    optimizer=my_optimizer,\n",
    "    metrics = [masked_acc],\n",
    ")\n",
    "\n",
    "# Llama al modelo con datos ficticios para construirlo\n",
    "dummy_input = tf.ones((1, 1), dtype=tf.int32)\n",
    "dummy_context = tf.ones((1, 1), dtype=tf.int32)\n",
    "model((dummy_context, dummy_input), training=False)\n",
    "\n",
    "model.load_weights(MODEL_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_translation(sentence, translated_sentence, ground_truth):\n",
    "  print(f'{\"Input:\":15s}: {sentence}')\n",
    "  print(f'{\"Prediction\":15s}: {translated_sentence}')\n",
    "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize_sentence(processor, sentence):\n",
    "  output_sentence = tf.constant('')\n",
    "  vocab = processor.get_vocabulary()\n",
    "  lookup = tf.constant('')\n",
    "\n",
    "  for token in sentence:\n",
    "    output_sentence = tf.strings.join([output_sentence, vocab[token]], separator=' ')\n",
    "\n",
    "  lookup = tf.strings.split(output_sentence, sep=' ')\n",
    "\n",
    "  sentence = output_sentence.numpy().decode('utf-8')\n",
    "  sentence = sentence.replace('[START]', '').replace('[END]', '').strip()\n",
    "\n",
    "  return sentence, lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, context_processor, target_processor, model, max_tokens=128):\n",
    "  # Convert the sentence to tensor\n",
    "  sentence = tf.constant([[sentence]])\n",
    "  assert isinstance(sentence, tf.Tensor)\n",
    "\n",
    "  # Give the correct shape\n",
    "  if len(sentence.shape) == 0:\n",
    "    sentence = sentence[tf.newaxis]\n",
    "  \n",
    "  # Tokenize the sentence\n",
    "  sentence = context_processor(sentence).to_tensor()\n",
    "\n",
    "  # Input for the encoder\n",
    "  encoder_input = sentence\n",
    "  \n",
    "  # Input for the decoder\n",
    "  start_end_tokens = target_processor([''])[0]\n",
    "  start_token = start_end_tokens[0][tf.newaxis]\n",
    "  end_token = start_end_tokens[-1][tf.newaxis]\n",
    "\n",
    "  # Convert decoder sentence to TensorArray for feed the model\n",
    "  output_tensor = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "  output_tensor = output_tensor.write(0, start_token)\n",
    "  \n",
    "  # seq_to_seq generation\n",
    "  for i in tf.range(max_tokens):\n",
    "\n",
    "    # Prepare the output tensor\n",
    "    output = tf.transpose(output_tensor.stack())\n",
    "    \n",
    "    # Get the model predictions\n",
    "    predictions = model((encoder_input, output), training=False)\n",
    "    \n",
    "    # Select the last token from the seq_len dimension\n",
    "    predictions = predictions[:, -1, :] # Shape (batch_size, 1, vocab_size)\n",
    "    # Get the most probable next token\n",
    "    predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "    # Concatenate the predicted token to the output sentence\n",
    "    output_tensor = output_tensor.write(i+1, predicted_id) #TODO: en el tuto es predicted_id[0]\n",
    "\n",
    "    # Check if the prediction is fully generated with the last token\n",
    "    if predicted_id == end_token:\n",
    "      break\n",
    "  \n",
    "  # Convert the output to the correct shape\n",
    "  output = tf.transpose(output_tensor.stack()) # Shape (1, tokens_generated)\n",
    "\n",
    "  # Detokenize the output sentence\n",
    "  text, lookup = detokenize_sentence(target_processor, output[0])\n",
    "\n",
    "  # 'tf.function' prevents us from using the attention_weights that were calculated on the\n",
    "  # last iteration of the loop. So, recalculate them outside the loop.\n",
    "  model((encoder_input, output[:, :-1]), training=False)\n",
    "  attention_weigths = model.decoder.last_attn_scores\n",
    "  #TODO: Check is attn_weigths is correct (with plotting function)\n",
    "\n",
    "  return text, lookup, attention_weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_translate_single_batch(sentences, context_processor, target_processor, model, max_tokens=128):\n",
    "    # Convert the sentences to a tensor\n",
    "    sentences = tf.constant(sentences)\n",
    "    assert isinstance(sentences, tf.Tensor)\n",
    "\n",
    "    # Tokenize the sentences\n",
    "    sentences = context_processor(sentences).to_tensor()\n",
    "\n",
    "    # Input for the encoder\n",
    "    encoder_input = sentences\n",
    "\n",
    "    # Input for the decoder\n",
    "    start_end_tokens = target_processor([''])[0]\n",
    "    start_token = start_end_tokens[0][tf.newaxis]\n",
    "    end_token = start_end_tokens[-1][tf.newaxis]\n",
    "    end_token_id = end_token.numpy()[0]\n",
    "    dot_token_id = target_processor.get_vocabulary().index('.')\n",
    "\n",
    "    # Prepare the initial output tensor\n",
    "    batch_size = tf.shape(encoder_input)[0]\n",
    "    initial_output = tf.repeat(start_token, repeats=batch_size, axis=0)\n",
    "\n",
    "    # Initialize the output tensor as a list of empty TensorArrays, one for each sentence\n",
    "    output_tensors = [tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True) for _ in range(batch_size)]\n",
    "\n",
    "    # Write the start token to each TensorArray\n",
    "    for i, tensor_array in enumerate(output_tensors):\n",
    "        tensor_array = tensor_array.write(0, initial_output[i])\n",
    "        tensor_array.mark_used()\n",
    "   \n",
    "\n",
    "    # Initialize a boolean tensor to track the completed sentences\n",
    "    completed_sentences = tf.zeros(batch_size, dtype=tf.bool)\n",
    "\n",
    "    for _ in tf.range(max_tokens):\n",
    "        # Prepare the output tensor\n",
    "        max_sentence_length = max([tensor_array.size() for tensor_array in output_tensors])\n",
    "        padded_output_tensors = [tf.pad(tensor_array.stack(), [[0, max_sentence_length - tensor_array.size()]]) for tensor_array in output_tensors]\n",
    "        output = tf.stack(padded_output_tensors, axis=0)\n",
    "\n",
    "        # Get the model predictions\n",
    "        predictions = model((encoder_input, output), training=False)\n",
    "\n",
    "        # Select the last token from the seq_len dimension\n",
    "        predictions = predictions[:, -1, :]  # Shape (batch_size, 1, vocab_size)\n",
    "\n",
    "        # Get the most probable next token\n",
    "        predicted_ids = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "        # Concatenate the predicted token to the output sentence\n",
    "        for i, (tensor_array, predicted_id) in enumerate(zip(output_tensors, predicted_ids)):\n",
    "            if not completed_sentences[i]:\n",
    "                if predicted_id.numpy() == end_token_id or predicted_id == dot_token_id:\n",
    "                    completed_sentences = tf.where(tf.range(batch_size) == i, True, completed_sentences)\n",
    "                    output_tensors[i] = tensor_array.write(tensor_array.size(), predicted_id)\n",
    "                else:\n",
    "                    output_tensors[i] = tensor_array.write(tensor_array.size(), predicted_id)\n",
    "            else:\n",
    "                # Keep the end token if the sentence is already completed\n",
    "                output_tensors[i] = tensor_array.write(tensor_array.size(), end_token_id)\n",
    "\n",
    "        # Check if all the predictions are fully generated with the last token\n",
    "        if tf.reduce_all(completed_sentences):\n",
    "            break\n",
    "    \n",
    "\n",
    "    # Convert the output to the correct shape\n",
    "    output = tf.stack([tensor_array.stack() for tensor_array in output_tensors], axis=0)  # Shape (batch_size, tokens_generated)\n",
    "\n",
    "    # Detokenize the output sentences\n",
    "    texts, lookups = [], []\n",
    "    for i in range(batch_size):\n",
    "        text, lookup = detokenize_sentence(target_processor, output[i])\n",
    "        texts.append(text)\n",
    "        lookups.append(lookup)\n",
    "\n",
    "    # 'tf.function' prevents us from using the attention_weights that were calculated on the\n",
    "    # last iteration of the loop. So, recalculate them outside the loop.\n",
    "    model((encoder_input, output[:, :-1]), training=False)\n",
    "    attention_weights = model.decoder.last_attn_scores\n",
    "\n",
    "    return texts, lookups, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_translate(sentences, context_processor, target_processor, model, max_tokens=128, batch_size=16):\n",
    "    num_batches = len(sentences) // batch_size + int(len(sentences) % batch_size > 0)\n",
    "    all_translated_texts = []\n",
    "    # all_lookups = []\n",
    "    # all_attention_weights = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        print('Batch starting... ' + str(i+1) + '/' + str(num_batches))\n",
    "        batch_start = i * batch_size\n",
    "        batch_end = min((i + 1) * batch_size, len(sentences))\n",
    "        batch_sentences = sentences[batch_start:batch_end]\n",
    "        \n",
    "        # Call the previous batch_translate function with the smaller batch of sentences\n",
    "        translated_text, lookup, attention_weights = batch_translate_single_batch(batch_sentences, context_processor, target_processor, model, max_tokens)\n",
    "        \n",
    "        all_translated_texts.extend(translated_text)\n",
    "        # all_lookups.extend(lookup)\n",
    "        # all_attention_weights.append(attention_weights)\n",
    "\n",
    "    return all_translated_texts, _, __\n",
    "    # return all_translated_texts, all_lookups, all_attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 9)\n",
      "tf.Tensor([[  3  32 150   9  21  26 171 284   4]], shape=(1, 9), dtype=int64)\n",
      "Input:         : Quiero estar en casa hoy\n",
      "Prediction     : i want to be at home today\n",
      "Ground truth   : I want to be at home today\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Quiero estar en casa hoy'\n",
    "ground_truth = 'I want to be at home today'\n",
    "\n",
    "translated_text, lookup, attention_weigths = translate(sentence, context_text_processor, target_text_processor, model)\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : Me gusta beber agua en el parque\n",
      "Prediction     : i like to drink water at the park\n",
      "Ground truth   : I like to drink water in the park\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Me gusta beber agua en el parque'\n",
    "ground_truth = 'I like to drink water in the park'\n",
    "\n",
    "translated_text, lookup, attention_weigths = translate(sentence, context_text_processor, target_text_processor, model)\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : La universidad me ha enseñado que la vida no consiste sólo en tener conocimientos técnicos, sino también en conocer a gente que puede ayudarte en el futuro.\n",
      "Prediction     : the university has taught me that life is not only in having technical knowledge , but also know people who can help in future .\n",
      "Ground truth   : university has taught me that life is not only about technical knowledge, but also about meeting people who can help you in the future.\n"
     ]
    }
   ],
   "source": [
    "sentence = 'La universidad me ha enseñado que la vida no consiste sólo en tener conocimientos técnicos, sino también en conocer a gente que puede ayudarte en el futuro.'\n",
    "ground_truth = 'university has taught me that life is not only about technical knowledge, but also about meeting people who can help you in the future.'\n",
    "\n",
    "translated_text, lookup, attention_weigths = translate(sentence, context_text_processor, target_text_processor, model)\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_beams(beam, model, encoder_input, beam_width):\n",
    "    tokens, score = beam\n",
    "\n",
    "    # Convert tokens to tensor\n",
    "    output = tf.convert_to_tensor([tokens])\n",
    "\n",
    "    # Get the model predictions\n",
    "    predictions = model((encoder_input, output), training=False)\n",
    "    \n",
    "    # Select the last token from the seq_len dimension\n",
    "    predictions = predictions[0, -1, :]  # Shape (vocab_size,)\n",
    "\n",
    "    # Get the beam_width most probable next tokens\n",
    "    predicted_ids = tf.math.top_k(predictions, k=beam_width).indices.numpy()\n",
    "    \n",
    "    # Calculate log probabilities of the most probable tokens\n",
    "    log_probs = tf.math.log(tf.nn.softmax(predictions))\n",
    "    # Get the log probabilities of the predicted_ids\n",
    "    predicted_log_probs = tf.gather(log_probs, predicted_ids)\n",
    "\n",
    "\n",
    "    new_beams = []\n",
    "    for predicted_id, predicted_log_prob in zip(predicted_ids, predicted_log_probs):\n",
    "        new_beam = (list(tokens) + [predicted_id], score + predicted_log_prob.numpy())\n",
    "        new_beams.append(new_beam)\n",
    "\n",
    "    return new_beams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_beam(sentence, context_processor, target_processor, model, max_tokens=64, beam_width = 5):\n",
    "  # Convert the sentence to tensor\n",
    "  sentence = tf.constant([[sentence]])\n",
    "  assert isinstance(sentence, tf.Tensor)\n",
    "\n",
    "  # Give the correct shape\n",
    "  if len(sentence.shape) == 0:\n",
    "    sentence = sentence[tf.newaxis]\n",
    "  \n",
    "  # Tokenize the sentence\n",
    "  sentence = context_processor(sentence).to_tensor()\n",
    "\n",
    "  # Input for the encoder\n",
    "  encoder_input = sentence\n",
    "  \n",
    "  # Input for the decoder\n",
    "  start_end_tokens = target_processor([''])[0]\n",
    "  start_token = start_end_tokens[0][tf.newaxis]\n",
    "  end_token = start_end_tokens[-1][tf.newaxis]\n",
    "  dot_token = tf.convert_to_tensor([target_processor.get_vocabulary().index('.')], dtype=tf.int64)\n",
    "\n",
    "  # Convert decoder sentence to TensorArray for feed the model\n",
    "  output_tensor = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "  output_tensor = output_tensor.write(0, start_token)\n",
    "\n",
    "  # Define params for beam search\n",
    "  active_beams = [(start_token.numpy(), 0.0)]  # Each beam is a pair of (tokens, score)\n",
    "  final_beams = []\n",
    "\n",
    "  for i in tf.range(max_tokens):\n",
    "      next_beams = []\n",
    "      for beam in active_beams:\n",
    "          # print(beam)\n",
    "          # Expand the current beam by all possible next tokens\n",
    "          expanded_beams = expand_beams(beam, model, encoder_input, beam_width)\n",
    "\n",
    "          # Add the expanded beams to next_beams\n",
    "          next_beams.extend(expanded_beams)\n",
    "          \n",
    "\n",
    "\n",
    "      # Prune down to the beam_width highest scoring beams\n",
    "      next_beams.sort(key=lambda x: x[1], reverse=True)\n",
    "      active_beams = next_beams[:beam_width]\n",
    "\n",
    "      \n",
    "      # Check if any of the active beams have produced an end token\n",
    "      # completed_beams = [(beam for beam in active_beams if beam[0][-1] == end_token) or (beam for beam in active_beams if beam[0][-1] == dot_token)]\n",
    "      completed_beams = [beam for beam in active_beams if (beam[0][-1] == end_token or beam[0][-1] == dot_token)]\n",
    "      active_beams = [beam for beam in active_beams if beam not in completed_beams]\n",
    "      \n",
    "      # Add the completed beams to a final hypotheses list\n",
    "      final_beams.extend(completed_beams)\n",
    "      \n",
    "      # If there are enough final hypotheses, break\n",
    "      if len(final_beams) >= beam_width:\n",
    "          break\n",
    "  \n",
    "  # Sort the final output by the log error\n",
    "  final_beams.sort(key=lambda x: x[1], reverse=True)\n",
    "  # Get the best translation\n",
    "  output = final_beams[0][0]\n",
    "  output = tf.convert_to_tensor([output], dtype=tf.int64)\n",
    "\n",
    "  # Detokenize the output sentence\n",
    "  text, lookup = detokenize_sentence(target_processor, output[0])\n",
    "\n",
    "  # 'tf.function' prevents us from using the attention_weights that were calculated on the\n",
    "  # last iteration of the loop. So, recalculate them outside the loop.\n",
    "  model((encoder_input, output[:, :-1]), training=False)\n",
    "  attention_weigths = model.decoder.last_attn_scores\n",
    "  #TODO: Check is attn_weigths is correct (with plotting function)\n",
    "\n",
    "  return text, lookup, attention_weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : Quiero estar en casa hoy\n",
      "Prediction     : i want to be at home today\n",
      "Ground truth   : I want to be at home today\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Quiero estar en casa hoy'\n",
    "ground_truth = 'I want to be at home today'\n",
    "\n",
    "translated_text, lookup, attention_weigths = translate_beam(sentence, context_text_processor, target_text_processor, model)\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : Me gusta beber agua en el parque\n",
      "Prediction     : i like to drink water at the park\n",
      "Ground truth   : I like to drink water in the park\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Me gusta beber agua en el parque'\n",
    "ground_truth = 'I like to drink water in the park'\n",
    "\n",
    "translated_text, lookup, attention_weigths = translate_beam(sentence, context_text_processor, target_text_processor, model)\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : La universidad me ha enseñado que la vida no consiste sólo en tener conocimientos técnicos, sino también en conocer a gente que puede ayudarte en el futuro.\n",
      "Prediction     : the university has taught me that life is not only in having technical knowledge , but also know people who can help in future .\n",
      "Ground truth   : university has taught me that life is not only about technical knowledge, but also about meeting people who can help you in the future.\n"
     ]
    }
   ],
   "source": [
    "sentence = 'La universidad me ha enseñado que la vida no consiste sólo en tener conocimientos técnicos, sino también en conocer a gente que puede ayudarte en el futuro.'\n",
    "ground_truth = 'university has taught me that life is not only about technical knowledge, but also about meeting people who can help you in the future.'\n",
    "\n",
    "translated_text, lookup, attention_weigths = translate_beam(sentence, context_text_processor, target_text_processor, model)\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open file E:\\TFG\\eval\n",
    "with open('eval/europarl-v7.it-en.en', 'r') as f:\n",
    "    data_en = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eval/europarl-v7.it-en.it', 'r') as f:\n",
    "    data_it = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data_en[-256:]\n",
    "b = data_it[-256:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_sentences, _, __ = batch_translate(a, context_text_processor, target_text_processor, model, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esp = [x.lower() for x in b]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu_score(references, predictions):\n",
    "    # Tokenize the sentences\n",
    "    tokenized_references = [word_tokenize(ref) for ref in references]\n",
    "    tokenized_predictions = [word_tokenize(pred) for pred in predictions]\n",
    "\n",
    "    # Calculate the BLEU score using the NLTK library\n",
    "    bleu_score = bleu.corpus_bleu([[ref] for ref in tokenized_references], tokenized_predictions)\n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_score = calculate_bleu_score(b, translated_sentences)\n",
    "print(\"BLEU Score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_score = calculate_bleu_score(esp, translated_sentences)\n",
    "print(\"BLEU Score:\", bleu_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meteor score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_meteor_score(references, predictions):\n",
    "    # Tokenize the sentences\n",
    "    tokenized_references = [word_tokenize(ref) for ref in references]\n",
    "    tokenized_predictions = [word_tokenize(pred) for pred in predictions]\n",
    "\n",
    "    # Calculate the METEOR score\n",
    "    scores = [single_meteor_score(ref, pred) for ref, pred in zip(tokenized_references, tokenized_predictions)]\n",
    "    average_score = sum(scores) / len(scores)\n",
    "\n",
    "    return average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor_score = calculate_meteor_score(b, translated_sentences)\n",
    "print(\"METEOR Score:\", meteor_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor_score = calculate_meteor_score(esp, translated_sentences)\n",
    "print(\"METEOR Score:\", meteor_score)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
